r"""
extend sentences by mask and filling sentence
==========================================================
"""

__all__ = ['MLMSentenceSuggestion']

import spacy
import torch
import random
import string
import difflib

from textflint.generation.transformation import Transformation


class MLMSentenceSuggestion(Transformation):
    r"""
    Extend sentences by BART generated sentences.

    Example::

        given "sentence1 sentence2 sentence3 ..."
    ->  prefix_sentences + "mask random sentence and generated by BART" + suffix_sentences

    """

    def __init__(
        self,
        num_candidate=10,
        max_word_len=20,
        max_match_score=0.5,
        seed=42,
        device="cuda",
        **kwargs
    ):
        super().__init__()
        self.num_candidate = num_candidate
        self.max_word_len = max_word_len
        self.max_match_score = max_match_score

        self.device = self.get_device(device)
        self.get_model()

        self.rng = random.Random(seed)
        self.nlp = spacy.load("en_core_web_sm")
        self.mlm_prompts = [
            "We know that", "We can say that", "It is about that", "It is known that", "The fact is that", "The answer is that", "In accordance with", "As per", "In light of", "In line with", "On the basis of", "In agreement with", "Based on", "As evidenced by", "Per the data", "Per the information"
        ]

    @staticmethod
    def get_device(device):
        r"""
        Get gpu or cpu device.
        :param str device: device string
                           "cpu" means use cpu device.
                           "cuda:0" means use gpu device which index is 0.
        :return: device in torch.
        """
        if not torch.cuda.is_available() or "cuda" not in device:
            return torch.device("cpu")
        else:
            return torch.device(device)

    def get_model(self):
        r"""
        Loads masked language model to predict candidates.

        """
        from transformers import AutoTokenizer, BartForConditionalGeneration

        self.tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")
        self.model = BartForConditionalGeneration.from_pretrained("facebook/bart-base", forced_bos_token_id=0).to(self.device)
        self.model.eval()

    def __repr__(self):
        return "MLMSentenceSuggestion"

    def _get_mlm_prompt(self):
        return self.rng.choice(self.mlm_prompts)

    def _check_sentence_filling(self, prefix, suffix, span, ):
        span = span.lower()
        prefix = prefix.lower()
        suffix = suffix.lower()

        if len(span) < 1 or span in string.punctuation:
            return False
        if prefix in span:  ## assert not copy query
            return False
        if len(span.split()) > self.max_word_len:  ## the max word num
            return False

        match_radio_query = difflib.SequenceMatcher(None, span, prefix).find_longest_match(0, len(span), 0, len(prefix)).size / len(span)
        match_radio_body = difflib.SequenceMatcher(None, span, suffix).find_longest_match(0, len(span), 0, len(suffix)).size / len(span)
        if match_radio_query > self.max_match_score or match_radio_body > self.max_match_score:
            return False

        return True

    def _get_sentence_filling(self, text, prefix, suffix, prompt):
        candidates = list()
        span_set = set()

        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        generated_ids = self.model.generate(
            inputs.input_ids,
            max_length=1024,
            num_beams=1,
            top_k=50, top_p=0.95, do_sample=True,
            num_return_sequences=self.num_candidate
        )
        gened = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        for gen_text in gened:
            gen_body_sents = [str(s).strip() for s in self.nlp(gen_text).sents]
            for sent in gen_body_sents:
                if sent.lower().startswith(prompt.lower()) and sent not in suffix:
                    span = sent[len(prompt):].strip()  ## delete the prefix
                    if not self._check_sentence_filling(prefix, suffix, span):
                        continue
                    if span[0].islower():
                        span = span[0].upper() + span[1:]
                    if span not in span_set:
                        candidates.append(" ".join([prefix, span, suffix]))
                        span_set.add(span)

        return candidates

    def _transform(self, sample, field='x', n=1, **kwargs):
        r"""
        Transform text string according transform_field.

        :param ~Sample sample:  input data, normally one data component.
        :param str field: indicate which field to transform
        :param int n: number of generated samples
        :param kwargs:
        :return list trans_samples: transformed sample list.
        """

        sents = sample.get_sentences(field)
        if len(sents) <= 1:
            print(f"WARNING!! only one sentence in origin document, return without modified -> {sents}")
            return [sample, ]
        rnd_sent_id = self.rng.choice(list(range(1, len(sents)))) # avoid the first sentence

        # rnd_sent = sents[rnd_sent_id]
        prefix = " ".join(sents[:rnd_sent_id]) # '' if rnd_sent_id == 0
        suffix = " ".join(sents[rnd_sent_id+1:]) # as same
        prompt = self._get_mlm_prompt()
        text = prefix + f" {prompt} <mask> " + suffix

        trans_candidates = self._get_sentence_filling(text, prefix, suffix, prompt)
        if len(trans_candidates) == 0:
            trans_candidates.append(prefix + " " + suffix)
            print(f"WARNING!! no candidates generated, remove random sentence as result -> {text}")

        trans_samples = list()
        for candidate in trans_candidates[:n]:
            trans_samples.append(
                sample.replace_field(field, candidate)
            )
        return trans_samples
